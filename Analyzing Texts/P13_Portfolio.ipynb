{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Дальнейшие-действия\" data-toc-modified-id=\"Дальнейшие-действия-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Дальнейшие действия</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Задача: обучить модель классифицировать комментарии на позитивные и негативные и построить модель со значением метрики качества *F1* не меньше 0.75. </b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала отдельно импортируем все необходимые библиотеки и функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее откроем и изучим файл с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков нет, но избавимся от бесполезного столбца 'Unnamed: 0'. Сохраним значение переменной гиперпараметра random_state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "rand_state = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем большой класс, реализующий сразу все этапы предобработки и классификации комментариев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class comments_classification():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 models_and_params: list,\n",
    "                 score: str,\n",
    "                 solvers: list,\n",
    "                 stop_words,\n",
    "                 start_frame,\n",
    "                 target_column,\n",
    "                 data_column):\n",
    "        # Инициализация всех переданных параметров\n",
    "        self.models_and_params = models_and_params\n",
    "        self.score = score\n",
    "        self.solvers = solvers\n",
    "        self.stop_words = stop_words\n",
    "        self.start_frame = start_frame\n",
    "        self.target_column = target_column\n",
    "        self.data_column = data_column\n",
    "        self.lemm_corpus = None\n",
    "        self.splited_data = None\n",
    "        self.vect = None\n",
    "        self.max_score = -1\n",
    "        self.best_model = None\n",
    "        \n",
    "        # preprocessing\n",
    "        self.__text_clearning()\n",
    "        print(\"Первый этап пройден\")\n",
    "        self.__lemmatisation()\n",
    "        print(\"Второй этап пройден\")\n",
    "        self.__splitter()\n",
    "        print(\"Третий этап пройден\")\n",
    "        self.__vectorisation()\n",
    "        print(\"Подготовка завершена\")\n",
    "\n",
    "        \n",
    "    def __splitter(self):\n",
    "        \"\"\" Функция, разделяющая обработанные данные на тренировочную,\n",
    "        валидационную и тестовую\n",
    "        \"\"\"\n",
    "\n",
    "        presplited_data = train_test_split(\n",
    "            self.lemm_corpus, self.start_frame[self.target_column], test_size = 0.2, random_state = rand_state)\n",
    "        splited_data_w_val = train_test_split(\n",
    "            presplited_data[1], presplited_data[3], test_size = 0.5, random_state = rand_state)\n",
    "        self.splited_data = [presplited_data[0], splited_data_w_val[0], splited_data_w_val[1],\n",
    "                             presplited_data[2], splited_data_w_val[2], splited_data_w_val[3]]\n",
    "\n",
    "        \n",
    "    def __lemmatisation(self):\n",
    "        \"\"\"Функция, отвечающая за лемматизацию слов корпуса\n",
    "        \"\"\"\n",
    "        # Инициализация лемматизатора\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Лемматизация корпуса\n",
    "        self.lemm_corpus = self.corpus.apply(\n",
    "            lambda sentence: \" \".join([lemmatizer.lemmatize(word, \"n\") for word in nltk.word_tokenize(sentence)]))\n",
    "\n",
    "        \n",
    "    def __text_clearning(self):\n",
    "        \"\"\" Функция, отвечающая за очистку корпуса от лишних символов\n",
    "        \"\"\"\n",
    "        # Выделение корпуса для дальнейшего анализа\n",
    "        corpus = self.start_frame[self.data_column]\n",
    "        # Очистка корпуса от лишних символов\n",
    "        self.corpus = corpus.apply(lambda sentence: re.sub(r'[^a-zA-Z]', ' ', sentence))\n",
    "        \n",
    "    def __vectorisation(self):\n",
    "        \"\"\"Функция, отвечающая за векторизацию корпуса\n",
    "        \"\"\"\n",
    "        # Создание словаря со словарями, которые хранят в себе векторизованные данные от разных векторизаторов \n",
    "        self.vect = {str(i()):{} for i in self.solvers}\n",
    "        # Векторизация данных разными методами\n",
    "        for vectorizer in self.solvers:\n",
    "            # Инициализация векторизатора и установка стоп-слов\n",
    "            vector = vectorizer(stop_words = self.stop_words)\n",
    "            # Обучение и трансформация на обучающей выборке\n",
    "            self.vect[str(vectorizer())]['train_data'] = vector.fit_transform(self.splited_data[0])\n",
    "            self.vect[str(vectorizer())]['train_target'] = self.splited_data[3]\n",
    "            # Трансформация тестовой выборки\n",
    "            self.vect[str(vectorizer())]['test_data'] = vector.transform(self.splited_data[1])\n",
    "            self.vect[str(vectorizer())]['test_target'] = self.splited_data[4]\n",
    "            # Трансформация валидационной выборки\n",
    "            self.vect[str(vectorizer())]['valid_data'] = vector.transform(self.splited_data[2])\n",
    "            self.vect[str(vectorizer())]['valid_target'] = self.splited_data[5]\n",
    "            \n",
    "            \n",
    "    def fit(self):\n",
    "        \"\"\"Тренируем все переданные модели\n",
    "        \"\"\"\n",
    "        # Инициализация словаря\n",
    "        self.black_boxes = {str(name):{} for name,_ in self.models_and_params}\n",
    "        # Перебор всех моделей\n",
    "        for model, params in tqdm(self.models_and_params):\n",
    "            # Перебор всех векторизаторов\n",
    "            for vectorizer, data in tqdm(self.vect.items(), desc = str(model)):\n",
    "                # Инициализация внутреннего словаря\n",
    "                self.black_boxes[str(model)][str(vectorizer)] = {}\n",
    "                # Инициализация грида\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"] = GridSearchCV(\n",
    "                    model, params, cv=3, scoring=self.score)\n",
    "                # Тренировка грида\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"grid_object\"].fit(\n",
    "                    self.vect[str(vectorizer)]['train_data'], self.vect[str(vectorizer)]['train_target'])\n",
    "                # Сохранение лучшей модели\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_model\"] = self.black_boxes[\n",
    "                    str(model)][str(vectorizer)][\"grid_object\"].best_estimator_\n",
    "                # Сохранение лучшего скора на разных выборках\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_train\"] = self.black_boxes[\n",
    "                    str(model)][str(vectorizer)][\"grid_object\"].best_score_\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_valid\"] = f1_score(\n",
    "                    self.vect[str(vectorizer)]['valid_target'], self.black_boxes[\n",
    "                        str(model)][str(vectorizer)][\"best_model\"].predict(self.vect[str(vectorizer)]['valid_data']))\n",
    "                self.black_boxes[str(model)][str(vectorizer)][\"best_score_test\"] = f1_score(\n",
    "                    self.vect[str(vectorizer)]['test_target'],self.black_boxes[\n",
    "                        str(model)][str(vectorizer)][\"best_model\"].predict(self.vect[str(vectorizer)]['test_data']))\n",
    "                # Поиск максимального скора на валидационной выборке\n",
    "                if self.black_boxes[str(model)][str(vectorizer)][\"best_score_valid\"] > self.max_score:\n",
    "                    self.max_score = self.black_boxes[str(model)][str(vectorizer)][\"best_score_test\"]\n",
    "                    self.best_model = self.black_boxes[str(model)][str(vectorizer)][\"best_model\"]\n",
    "                    \n",
    "        return {\"max_score\":self.max_score, \"best_model\":self.best_model}\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\" Функция, возвращающая всю собранную информацию об обучении\n",
    "        \"\"\"\n",
    "        return self.black_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого можно сразу приступить к обучению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим параметры для модели и затем инициализируем класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первый этап пройден\n",
      "Второй этап пройден\n",
      "Третий этап пройден\n",
      "Подготовка завершена\n",
      "CPU times: user 1min 37s, sys: 424 ms, total: 1min 37s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "params_logistic = {\"max_iter\": [1000, 2000, 100]}\n",
    "params_random_forest = {\"n_estimators\": [40, 200, 20],\"max_depth\": [2, 10]}\n",
    "\n",
    "comments_class = comments_classification(\n",
    "    [(LogisticRegression(random_state = rand_state, class_weight='balanced'), params_logistic),\n",
    "     (RandomForestClassifier(random_state = rand_state, class_weight='balanced'), params_random_forest)],\n",
    "    'f1', [TfidfVectorizer, CountVectorizer], stop_words, df, 'toxic', 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём наилучшую модель и метрику качества F1 для неё, используя специальную функцию класса, определённого выше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      "LogisticRegression(class_weight='balanced', random_state=12345):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "LogisticRegression(class_weight='balanced', random_state=12345):  50%|█████     | 1/2 [07:22<07:22, 442.41s/it]\u001b[A\n",
      "LogisticRegression(class_weight='balanced', random_state=12345): 100%|██████████| 2/2 [33:56<00:00, 1018.36s/it]\u001b[A\n",
      " 50%|█████     | 1/2 [33:56<33:56, 2036.72s/it]\n",
      "RandomForestClassifier(class_weight='balanced', random_state=12345):   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "RandomForestClassifier(class_weight='balanced', random_state=12345):  50%|█████     | 1/2 [02:47<02:47, 167.40s/it]\u001b[A\n",
      "RandomForestClassifier(class_weight='balanced', random_state=12345): 100%|██████████| 2/2 [05:31<00:00, 165.79s/it]\u001b[A\n",
      "100%|██████████| 2/2 [39:28<00:00, 1184.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 44s, sys: 18min 41s, total: 39min 26s\n",
      "Wall time: 39min 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_score': 0.763172804532578,\n",
       " 'best_model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=12345)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comments_class.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим подробнее всю собранную информацию об обучении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"LogisticRegression(class_weight='balanced', random_state=12345)\": {'TfidfVectorizer()': {'grid_object': GridSearchCV(cv=3,\n",
       "                estimator=LogisticRegression(class_weight='balanced',\n",
       "                                             random_state=12345),\n",
       "                param_grid={'max_iter': [1000, 2000, 100]}, scoring='f1'),\n",
       "   'best_model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=12345),\n",
       "   'best_score_train': 0.7452635998983105,\n",
       "   'best_score_valid': 0.7466954410574588,\n",
       "   'best_score_test': 0.7514200703272925},\n",
       "  'CountVectorizer()': {'grid_object': GridSearchCV(cv=3,\n",
       "                estimator=LogisticRegression(class_weight='balanced',\n",
       "                                             random_state=12345),\n",
       "                param_grid={'max_iter': [1000, 2000, 100]}, scoring='f1'),\n",
       "   'best_model': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=12345),\n",
       "   'best_score_train': 0.7551114973464736,\n",
       "   'best_score_valid': 0.754566210045662,\n",
       "   'best_score_test': 0.763172804532578}},\n",
       " \"RandomForestClassifier(class_weight='balanced', random_state=12345)\": {'TfidfVectorizer()': {'grid_object': GridSearchCV(cv=3,\n",
       "                estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                 random_state=12345),\n",
       "                param_grid={'max_depth': [2, 10], 'n_estimators': [40, 200, 20]},\n",
       "                scoring='f1'),\n",
       "   'best_model': RandomForestClassifier(class_weight='balanced', max_depth=10, n_estimators=200,\n",
       "                          random_state=12345),\n",
       "   'best_score_train': 0.36215565539913563,\n",
       "   'best_score_valid': 0.3661273486430062,\n",
       "   'best_score_test': 0.3647165181454217},\n",
       "  'CountVectorizer()': {'grid_object': GridSearchCV(cv=3,\n",
       "                estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                                 random_state=12345),\n",
       "                param_grid={'max_depth': [2, 10], 'n_estimators': [40, 200, 20]},\n",
       "                scoring='f1'),\n",
       "   'best_model': RandomForestClassifier(class_weight='balanced', max_depth=10, n_estimators=200,\n",
       "                          random_state=12345),\n",
       "   'best_score_train': 0.35359665287020725,\n",
       "   'best_score_valid': 0.356219405045814,\n",
       "   'best_score_test': 0.35713389383862465}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_class.get_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что наилучшее качество метрики F1 получено при использовании модели логистической регрессии, сбалансированную по классам и количеством итераций, равным 1000, а также с использованием векторизатора CountVectorizer (мешок слов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимая модель найдена, метрика качества F1, полученная на тестовой выборке, равна 0.763 и удовлетворяет условию нашей задачи. Лучшая модель основанна на алгоритме LogisticRegression с использованием балансировки классов и количеством итераций, равным 1000, а также с использованием векторизатора CountVectorizer (мешок слов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальнейшие действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли пути для улучшения результата? Да, есть.\n",
    "\n",
    "Можно сделать следующее:\n",
    "1. Провести векторизацию методом BERT. Это потребует серьёзных вычислительных мощностей.\n",
    "2. Использовать RNN (LSTM)."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3652,
    "start_time": "2022-09-16T04:11:17.857Z"
   },
   {
    "duration": 72,
    "start_time": "2022-09-16T05:01:06.568Z"
   },
   {
    "duration": 1726,
    "start_time": "2022-09-16T05:02:34.230Z"
   },
   {
    "duration": 800,
    "start_time": "2022-09-16T05:02:45.493Z"
   },
   {
    "duration": 10,
    "start_time": "2022-09-16T05:03:49.732Z"
   },
   {
    "duration": 21,
    "start_time": "2022-09-16T06:08:24.970Z"
   },
   {
    "duration": 94036,
    "start_time": "2022-09-16T06:08:32.937Z"
   },
   {
    "duration": 439304,
    "start_time": "2022-09-16T06:11:09.541Z"
   },
   {
    "duration": 1795,
    "start_time": "2022-09-16T08:00:19.113Z"
   },
   {
    "duration": 915,
    "start_time": "2022-09-16T08:00:20.910Z"
   },
   {
    "duration": 16,
    "start_time": "2022-09-16T08:00:21.827Z"
   },
   {
    "duration": 125,
    "start_time": "2022-09-16T08:00:21.846Z"
   },
   {
    "duration": 98002,
    "start_time": "2022-09-16T08:00:21.973Z"
   },
   {
    "duration": 2368319,
    "start_time": "2022-09-16T08:01:59.977Z"
   },
   {
    "duration": 15,
    "start_time": "2022-09-16T08:41:28.298Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
